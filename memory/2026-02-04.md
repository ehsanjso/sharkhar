# 2026-02-04 — Tuesday

## 02:00 PM — Daily Research: Ollama & Local LLMs on Raspberry Pi

**Topic selected:** Running local LLMs with Ollama on Raspberry Pi 5

**Why this topic:**
- Natural progression from HA AI research (Feb 3) — Ollama mentioned as conversation agent
- Fits self-hosted infrastructure theme (Pi-hole, Uptime Kuma, Mission Control)
- Potential quota relief — use local models for simple tasks, save Claude for complex work
- Privacy benefits — keep sensitive queries completely local
- Pi 5's 8GB RAM can comfortably run 1-3B parameter models

**Key discoveries:**
1. **Easy install** — Single curl command: `curl -fsSL https://ollama.com/install.sh | sh`
2. **Viable on Pi 5** — 1-3B models run at 5-15 tokens/sec (usable for chat)
3. **Home Assistant native** — HA has built-in Ollama integration for conversation agents
4. **OpenAI-compatible API** — REST API at localhost:11434, easy to integrate
5. **Best small models** — tinyllama (638MB), llama3.2:1b (1.3GB), phi4-mini (2.5GB)
6. **Open WebUI** — ChatGPT-like interface available via Docker
7. **Hybrid approach** — Use Ollama for simple queries, Claude for complex reasoning

**Recommended models for Pi 5:**
| Model | Size | Use Case |
|-------|------|----------|
| tinyllama | 638MB | Quick Q&A, fastest |
| llama3.2:1b | 1.3GB | Best quality small model |
| phi4-mini | 2.5GB | Math, coding, reasoning |

**Practical applications:**
- Smart home voice control via HA (offline, instant response)
- Quick local queries without quota burn
- Privacy-sensitive tasks (finance, health)
- Simple cron job automation (log summarization, etc.)
- Local embeddings with nomic-embed-text

**Quick start:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull tinyllama
ollama run tinyllama "Hello!"
```

**Next steps:**
1. Install Ollama on Pi
2. Test performance with different models
3. Add to Home Assistant as conversation agent
4. Consider ClawdBot skill for local fallback

**Research saved:** `memory/research/2026-02-04-ollama-local-llms-raspberry-pi.md`

---

## 06:00 PM — Spare Capacity: ArXiv Research Scan

Quick scan of recent AI/ML papers during spare capacity work (all quotas <50%).

**Notable papers:**
1. **PULSE** — 100x compression for distributed RL weight sync (99%+ sparsity)
2. **Gemini Deep Think** — Case studies on AI solving open math problems
3. **AutoFigure** — Agentic scientific illustration generation (ICLR 2026)
4. **Conformal Thinking** — Risk control for adaptive reasoning token budgets

**Research saved:** `memory/research/2026-02-04-arxiv.md`
